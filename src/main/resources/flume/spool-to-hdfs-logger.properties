# Flume configuration using a spooling directory source, fanning out to an HDFS sink and a logger sink

# Flume uses a separate transaction to deliver each batch of events from the spooling directory source to each channel
# If either of these transactions fails (if a channel is full, for example), then the events will not be removed from the source, and will be retried later.

agent1.sources = source1
agent1.sinks = sink1a sink1b
agent1.channels = channel1a channel1b

agent1.sources.source1.channels = channel1a channel1b
agent1.sources.source1.selector.type = replicating

# optional channel, so that if the transaction associated with it fails, this will not cause events to be left in the source and tried again later
agent1.sources.source1.selector.optional = channel1b
agent1.sinks.sink1a.channel = channel1a
agent1.sinks.sink1b.channel = channel1b

agent1.sources.source1.type = spooldir
agent1.sources.source1.spoolDir = /tmp/spooldir

agent1.sinks.sink1a.type = hdfs
agent1.sinks.sink1a.hdfs.path = /tmp/flume
agent1.sinks.sink1a.hdfs.filePrefix = events
agent1.sinks.sink1a.hdfs.fileSuffix = .log
agent1.sinks.sink1a.hdfs.fileType = DataStream

agent1.sinks.sink1b.type = logger

agent1.channels.channel1a.type = file

# memory channel
# we are logging events for debugging purposes and donâ€™t mind losing events on agent restart.
agent1.channels.channel1b.type = memory
